%==== KNITR ===================================================================%



%==== START ===================================================================%

\documentclass{report}\usepackage[]{graphicx}\usepackage[]{xcolor}
% maxwidth is the original width if it is less than linewidth
% otherwise use linewidth (to make sure the graphics do not exceed the margin)
\makeatletter
\def\maxwidth{ %
  \ifdim\Gin@nat@width>\linewidth
    \linewidth
  \else
    \Gin@nat@width
  \fi
}
\makeatother

\definecolor{fgcolor}{rgb}{0.345, 0.345, 0.345}
\newcommand{\hlnum}[1]{\textcolor[rgb]{0.686,0.059,0.569}{#1}}%
\newcommand{\hlstr}[1]{\textcolor[rgb]{0.192,0.494,0.8}{#1}}%
\newcommand{\hlcom}[1]{\textcolor[rgb]{0.678,0.584,0.686}{\textit{#1}}}%
\newcommand{\hlopt}[1]{\textcolor[rgb]{0,0,0}{#1}}%
\newcommand{\hlstd}[1]{\textcolor[rgb]{0.345,0.345,0.345}{#1}}%
\newcommand{\hlkwa}[1]{\textcolor[rgb]{0.161,0.373,0.58}{\textbf{#1}}}%
\newcommand{\hlkwb}[1]{\textcolor[rgb]{0.69,0.353,0.396}{#1}}%
\newcommand{\hlkwc}[1]{\textcolor[rgb]{0.333,0.667,0.333}{#1}}%
\newcommand{\hlkwd}[1]{\textcolor[rgb]{0.737,0.353,0.396}{\textbf{#1}}}%
\let\hlipl\hlkwb

\usepackage{framed}
\makeatletter
\newenvironment{kframe}{%
 \def\at@end@of@kframe{}%
 \ifinner\ifhmode%
  \def\at@end@of@kframe{\end{minipage}}%
  \begin{minipage}{\columnwidth}%
 \fi\fi%
 \def\FrameCommand##1{\hskip\@totalleftmargin \hskip-\fboxsep
 \colorbox{shadecolor}{##1}\hskip-\fboxsep
     % There is no \\@totalrightmargin, so:
     \hskip-\linewidth \hskip-\@totalleftmargin \hskip\columnwidth}%
 \MakeFramed {\advance\hsize-\width
   \@totalleftmargin\z@ \linewidth\hsize
   \@setminipage}}%
 {\par\unskip\endMakeFramed%
 \at@end@of@kframe}
\makeatother

\definecolor{shadecolor}{rgb}{.97, .97, .97}
\definecolor{messagecolor}{rgb}{0, 0, 0}
\definecolor{warningcolor}{rgb}{1, 0, 1}
\definecolor{errorcolor}{rgb}{1, 0, 0}
\newenvironment{knitrout}{}{} % an empty environment to be redefined in TeX

\usepackage{alltt}

\usepackage[left=2cm, right=2cm, top=1cm, bottom=2cm]{geometry}

% Font.


% Main packages.
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{booktabs} 
\usepackage{rotating} 
\usepackage{lmodern}

% Required for Table.


%%

\title{SML Project - Documentation}
\author{Tristan Leiter}
\date{\today}

%==== DOCUMENT START ==========================================================%

\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\begin{document}

\maketitle

%==== ABSTRACT ================================================================%

% \begin{abstract}
% This report demonstrates the integration of R code and its output within a LaTeX document using Sweave. It covers the basic structure of a report, including a summary, chapters with subchapters, and a bibliography.
% \end{abstract}

%==== Table of content ========================================================%

\tableofcontents
\newpage

%==== Introduction ============================================================%

\chapter{Introduction}

\section{Introduction}




%==== Chapter 1: General ======================================================%

\chapter{Exploratory Data Analysis}

\section{Overview}

\subsection{Description of the dataset}

For this SML project, we are using the wine-quality dataset, obtained from
Kaggle (https://www.kaggle.com/datasets/taweilo/wine-quality-dataset-balanced-classification).
It contains 21'000 observations with 11 features and 1 dependent variable, the wine quality.
Wine quality is an ordinal variable. It represents a ranking from 3 to 9, with
9 being the highest score and better than all scores below it. 

The dependent variable y is nicely balanced as it has the same distribution across
all quality variables. Each quality variable contains 3'000 observations (14.3\% of
all observations). Subsequently, we keep quality as a numeric value and set this up as
a regression task instead of a classification approach. This is done in order to
allow for better inference (assuming the true quality would be 5, a prediction of 9 would
be much worse than predicting a 4).

\subsection{Feature description}

\begin{table}[ht]
\centering
\caption{Dataset Variable Descriptions}
\label{tab:var_desc}
\begin{tabular}{l l l}
\toprule
\textbf{Variable} & \textbf{Description} & \textbf{Type} \\
\midrule
fixed\_acidity & Fixed acids (tartaric, malic, citric) & float64 \\
volatile\_acidity & Volatile acids (primarily acetic) & float64 \\
citric\_acid & Citric acid content & float64 \\
residual\_sugar & Remaining sugar after fermentation & float64 \\
chlorides & Salt content & float64 \\
free\_sulfur\_dioxide & Free SO2 (preservative) & float64 \\
total\_sulfur\_dioxide & Total SO2 (bound + free) & float64 \\
density & Density (related to alcohol/sugar) & float64 \\
pH & pH level (acidity indicator) & float64 \\
sulphates & Sulphates additive amount & float64 \\
alcohol & Alcohol percentage & float64 \\
is\_white & Binary indicator for wine type (1 = White, 0 = Red) & int64 \\
quality & Target: Quality score (3-9) & int64 \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Feature bimodality}

It can be observed that many features show a bimodal distribution (two different modes).
We can clearly see the two distinct peaks in the histogram. This indicates that
we observe a mixture of two different underlying distinct data generation processes.
For example, due to the dataset including red and white wines, which usually have
different, distinct properties.

For our further process, it essentially implies a hidden categorical feature (wine type:
red or white wine) that is not explicitly given in the dataset. It can be
argued that decision trees should handle this case rather wll as they can easily make
a split to seperate the two populations whilst linear models might struggle.

\subsection{Feature standardization}

The variances differ by several orders of magnitude (total sulfur dioxide vs density, pH value,...).
Standardization will be essential in order for our regularized linear models to work well and
to ensure that all features contribute equally to the model. Yet, we also sort of
loose some variation in the feature space.

\subsection{Multicollinearity}

Based on the correlation plot, multicollinearity is present in our dataset. 
Some features (free sulfur dioxide vs total sulfur dioxide or also density vs
residual sugar) are highly positively correlated. In this case, we can expect the
coefficients of linear models to be unstable as the model cannot properly differentiate
the linear effect of each feature. Also, a small change in the training data
could flip a coefficient from positive to negative, making interpretation very difficult.
In addition, redundant features can lead to an overfitting of the model as we
start to fit noise in the training data instead of the true signal. 

Tree based models should generally work well in this environment as they will simply pick one of
the correlated features to split on and ignore the others. It does not hurt their
predictive performance. Yet, it will be difficult for interpreting the importance
of each feature.

\subsection{Splitting the dataset}
We split the dataset in three subsets, the training set (50\% of the data),
a validation set (25\% of the data), and finally the test set with the remaining
25\%. Since the dependent variable is balanced, we do not require additional
tools, like stratified sampling.


%==== Chapter 2: Defining the loss-function ===================================%

\chapter{Loss-function}

%%% WILL BE CHANGED.

% \section{Root Mean Squared Error (RMSE)}
% 
% For this project, we decided to use root mean squared error (RMSE) as the key
% loss metric for both model validation and final performance assessment in the test
% set. We perform the following minimization:
% 
% \[
% RMSE = \sqrt{\frac{1}{N}\sum_{i=1}^{N} (y_i - \hat{y}_i)^2}
% \]
% 
% Where:
% 
% * $y_i$ is the actual quality score (e.g., 6).
% * $\hat{y}_i$ is the predicted score (e.g., 5.8).
% * $N$ is the number of observations.
% 
% Whilst wine quality is an ordinal variable, a standard multi-class classification
% problem (using e.g. accuracy) is suboptimal. Firstly, accuracy would ignore
% ordinality. Predicting a quality of 3 for a true quality of 8 is penalized the same as predicting
% a 7, even though 7 is much better than a 3.
% 
% RMSE also treats the target as a continous variable. If our model predicts a
% 59 for a true 6, it has a very small error, whereas 5.1 would result in a bigger
% error. This rewards models that get close to the true value, even though
% we can not observe a continous value (rather a discrete one).
% 
% RMSE is also easier to interprete as the target variable. An RMSE of 0.7
% would mean, on average, that our model's prediction are about 0.7
% quality points away from the actual score.
% 
% \section{RMSE vs MSE}
% We chose RMSE, because it scales back to the original units of our target variable.
% MSE is difficult to interpret by itself. An RMSE of 0.7 means, "on average, our model's prediction 
% is about 0.7 points away from the true wine quality rating." We use a metric
% that matches the scale of the actual data.

%==== Chapter 3: Regularized LMs ==============================================%

\chapter{Regularized Linear Models}

\section{Multinomial Logistic Regression (Lasso \& Ridge)}

\textbf{Implementation:} \texttt{glmnet} with \texttt{family = "multinomial"}

\subsubsection*{What is being optimized?}
The model minimizes the \textbf{Multinomial Negative Log-Likelihood} (also known as Cross-Entropy Loss), combined with a regularization term. The objective function to minimize is:
\[
-\frac{1}{N} \sum_{i=1}^{N} \sum_{k=1}^{K} y_{ik} \log(p_{ik}) + \lambda \cdot \text{Penalty}(\beta)
\]
Where:
\begin{itemize}
    \item $N$ is the number of observations.
    \item $K$ is the number of classes (wine quality 3, 4, \dots, 9).
    \item $y_{ik}$ is an indicator (1 if observation $i$ is in class $k$, 0 otherwise).
    \item $p_{ik}$ is the predicted probability that observation $i$ is in class $k$.
    \item $\lambda$ is the regularization parameter (tuned via cross-validation).
    \item \textbf{Lasso Penalty:} $\sum |\beta_j|$ (pushes coefficients to exactly zero).
    \item \textbf{Ridge Penalty:} $\sum \beta_j^2$ (shrinks coefficients towards zero).
\end{itemize}

\subsubsection*{How it works}
\begin{itemize}
    \item \textbf{Training:} The \texttt{glmnet} algorithm uses coordinate descent to find the coefficients ($\beta$) that minimize this joint loss function for a fixed $\lambda$.
    \item \textbf{Tuning:} We test multiple $\lambda$ values. For each, we calculate the \textbf{Accuracy} on the validation set. We select the $\lambda$ that \textbf{maximizes Validation Accuracy}.
\end{itemize}

%==== Chapter 4: Decision Trees ===============================================%

\chapter{Decision Trees}

\section{General Decision Trees}

A conventional decision tree (like CART) is a powerful model, but it has a major flaw: it's greedy and prone to overfitting.

% Greedy Approach: At each node, the tree scans all available features and all possible split points to find the single best split that most improves the model (e.g., minimizes Gini Impurity or, in your regression_case, Residual Sum of Squares).
% 
% Overfitting: Because it's greedy and tries to find the perfect split every time, it will continue splitting until it has perfectly (or near-perfectly) memorized the training data. This creates a very deep, complex tree that captures all the noise and quirks of the training set.

High Variance: As a result, if you slightly change the training data (e.g., add or remove a few rows), you can get a completely different tree. This instability is called high variance.

\section{Random Forest}

\textbf{Implementation:} \texttt{randomForest} with a factor response variable.

Random Forest does not strictly "minimize" a global loss function like regression models. Instead, it minimizes \textbf{impurity} at every split in every tree. For classification, the standard impurity measure is the \textbf{Gini Index}:
\[
Gini(t) = 1 - \sum_{k=1}^{K} (p_{tk})^2
\]
Where $p_{tk}$ is the proportion of training samples in node $t$ that belong to class $k$.

\begin{itemize}
    \item \textbf{Training:} Each individual tree tries to find splits that \textbf{maximize the decrease in Gini Impurity}. A split that perfectly separates classes has a Gini index of 0.
    \item \textbf{Tuning:} We tune the \texttt{mtry} hyperparameter. For each \texttt{mtry} value, we train a forest and evaluate it on the validation set. We select the \texttt{mtry} that \textbf{maximizes Validation Accuracy}.
\end{itemize}

\section{Gradient Boosting}

\textbf{Implementation:} \texttt{gbm} with \texttt{distribution = "multinomial"}

GBM explicitly minimizes the \textbf{Multinomial Negative Log-Likelihood} (Multinomial Deviance), the same core loss function as standard logistic regression, but it does so sequentially.
\[
Loss = - \sum_{i=1}^{N} \sum_{k=1}^{K} y_{ik} \log(p_{ik})
\]

\begin{itemize}
    \item \textbf{Training:} GBM builds trees sequentially. Each new tree is trained to predict the \textbf{negative gradient} of this loss function with respect to the current model's predictions. Effectively, each new tree tries to correct the probabilistic errors made by the ensemble so far.
    \item \textbf{Tuning:} We use K-fold Cross-Validation to tune the number of trees ($M$) and interaction depth ($J$). We select the combination that \textbf{minimizes the CV Multinomial Deviance}.
\end{itemize}

%==== Chapter 5: Additional Models ============================================%

\chapter{Additional Models}

\section{CatBoost}

CatBoost's primary advantages come from two novel techniques that solve common problems in standard gradient boosting: **target leakage** and **prediction shift**.

\textbf{Implementation:} \texttt{catboost} with \texttt{loss\_function = 'MultiClass'}

CatBoost minimizes the \textbf{MultiClass} loss, which is its implementation of the negative log-likelihood for multiclass problems (similar to GBM's deviance).

\begin{itemize}
    \item \textbf{Training:} Like standard GBM, it builds trees sequentially to minimize this loss. However, it uses \textbf{Ordered Boosting} to combat prediction shift and \textbf{Ordered Target Statistics} to handle categorical features (though we didn't have any in this specific dataset after pre-processing).
    \item \textbf{Tuning:} We explicitly set the \texttt{eval\_metric = 'Accuracy'}. During hyperparameter tuning, we select the best parameters (learning rate, depth, L2 regularization) that \textbf{maximize Validation Accuracy}.
\end{itemize}

%==== Chapter 6: Results ======================================================%

\chapter{Results}




%==== Anhang ==================================================================%

\appendix 

\chapter{Overview}

%==== END =====================================================================%

\end{document}
